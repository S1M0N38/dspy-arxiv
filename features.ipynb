{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "3g7UMaMuGhZt",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# DSPy-arXiv\n",
    "\n",
    "Given an arXiv paper from the Computer Science (cs) section,\\\n",
    "extract its subcategories (e.g., cs.AI, cs.IR, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pathlib\n",
    "\n",
    "# dspy framework\n",
    "import dspy\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "# arXiv utilites\n",
    "from arxiv.taxonomy import definitions\n",
    "\n",
    "# various paths of the project\n",
    "PATH_ROOT = pathlib.Path(\".\").parent\n",
    "PATH_DATASET = PATH_ROOT / \"dataset\"\n",
    "PATH_DATABASE = PATH_ROOT / \"database\"\n",
    "\n",
    "# ports where services are exposed\n",
    "PORT_LM = 11433\n",
    "\n",
    "# selected categories, i.e. just the ones from Computer Science (cs)\n",
    "CATEGORIES = {\n",
    "    cat: meta\n",
    "    for cat, meta in definitions.CATEGORIES_ACTIVE.items()\n",
    "    if cat.split(\".\")[0] == \"cs\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "5wc3E6-aHVi5",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We use the term **dataset** to refer to a small selection of papers that will be used to 'train' the pipeline.\n",
    "\n",
    "- 50 papers in `trainset` - used for pipeline training\n",
    "- 50 papers in `valset` - used for pipeline evaluation during training\n",
    "- 50 papers in `testset` - used for pipeline evaluation after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "To construct the dataset, please refer to the *database.ipynb* notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "4xa9DkssVrJm",
    "outputId": "266e4101-170d-46b4-9d49-d81c847024d8",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_example(example: dict) -> dspy.Example:\n",
    "    \"\"\"\n",
    "    Turn a paper (example) into dspy.Example.\n",
    "    \"\"\"\n",
    "    categories = set(example[\"categories\"].split(\" \")) & set(CATEGORIES)\n",
    "    return {\n",
    "        \"title\": example[\"title\"],\n",
    "        \"abstract\": example[\"abstract\"],\n",
    "        \"text\": example[\"text\"],\n",
    "        \"categories\": categories,\n",
    "        \"labels\": dspy.Example(categories=categories),\n",
    "    }\n",
    "\n",
    "\n",
    "trainset, valset, testset = [], [], []\n",
    "\n",
    "for path in (PATH_DATASET / \"trainset\").glob(\"*.json\"):\n",
    "    with open(path) as f:\n",
    "        example = dspy.Example(preprocess_example(json.load(f)))\n",
    "        example = example.with_inputs(\"title\", \"abstract\", \"text\", \"labels\")\n",
    "        trainset.append(example)\n",
    "\n",
    "for path in (PATH_DATASET / \"valset\").glob(\"*.json\"):\n",
    "    with open(path) as f:\n",
    "        example = dspy.Example(preprocess_example(json.load(f)))\n",
    "        example = example.with_inputs(\"title\", \"abstract\", \"text\")\n",
    "        valset.append(example)\n",
    "\n",
    "for path in (PATH_DATASET / \"testset\").glob(\"*.json\"):\n",
    "    with open(path) as f:\n",
    "        example = dspy.Example(preprocess_example(json.load(f)))\n",
    "        example = example.with_inputs(\"title\", \"abstract\", \"text\")\n",
    "        testset.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "13JR0wybbVgn",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Each datapoint (paper + paper metadata) is a `dspy.Example`,\\\n",
    "a dict-like structure with *inputs* ($x$) and *labels* ($y$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "13JR0wybbVgn",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- Inputs:\n",
    "  - `title`: Title of the paper.\n",
    "  - `abstract`: Abstract of the paper.\n",
    "  - `text`: Text body of the paper parsed from PDF with [arxiv2text](https://github.com/dsdanielpark/arxiv2text). (This is future work.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "13JR0wybbVgn",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- Labels:\n",
    "  - `categories`: Set of associated categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "hide_input": true,
    "id": "gTL1bTAKXOH6",
    "outputId": "acf93d87-8efd-4436-9158-6910d51efa7a",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(re.sub(r\"[\\s\\n]+\", \" \", valset[0].title), \"\\n\")\n",
    "print(re.sub(r\"[\\s\\n]+\", \" \", valset[0].abstract)[:400], \"...\\n\")\n",
    "print(valset[0].labels().categories, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Metrics\n",
    "\n",
    "**Metrics** are scalar values that quantify the performance of a pipeline with respect to a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def metric_fn(labels, preds, trace=None):\n",
    "    preds: list[str] | str = preds.categories\n",
    "    labels: list[str] = labels.categories\n",
    "\n",
    "    # We assume that predicted categories are sorted by relevance\n",
    "    # We selected top k predicted categories\n",
    "    k = min(len(labels), len(preds))\n",
    "    top_k_preds = preds[:k] if isinstance(preds, list) else [preds]\n",
    "\n",
    "    # ground-truth labels are alphabetically sorted\n",
    "    # so it make sense to look at the intesection with top_k_preds\n",
    "    top_k_pred_set: set[str] = set(top_k_preds)\n",
    "    lables_set: set[str] = set(labels)\n",
    "\n",
    "    score: float = len(top_k_pred_set & lables_set) / len(labels)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## DSPy\n",
    "\n",
    "The DSPy framework resembles PyTorch.\n",
    "\n",
    "- I/O interface\n",
    "- Modular structure\n",
    "- Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### I/O interface:\n",
    "- $(x, y)$ → pipeline → generated outputs\n",
    "- (`title & abstract`, `categories`) → pipeline → `preds`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Modular structure:\n",
    "\n",
    "In PyTorch:\n",
    "- Tensor/s → Module → Tensor/s\n",
    "- Tensor/s → Module → Module → ... → Module → Tensor/s\n",
    "- e.g. `Linear`, `Dropout`, `ReLU`...\n",
    "  \n",
    "In DSPy:\n",
    "- InputField/s → Module → OutputField/s\n",
    "- InputField/s → Module → Module → ... → Module → OutputField/s\n",
    "- e.g. `Predict`, `ChainOfThought`, `React`, custom, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### \"Optimization\"\n",
    "\n",
    "In PyTorch:\n",
    "- Define `loss`. e.g., `nn.MSELoss`, `nn.CrossEntropyLoss`, ...\n",
    "- Define `optimizer`. e.g., `optim.SGD`, `optim.Adam`, ...\n",
    "- Minimize `loss` over `trainset` using `optimizer` by adjusting model parameters.\n",
    "\n",
    "In DSPy:\n",
    "- Define `metric`. e.g., `metric_fn`\n",
    "- Define `optimizer`. e.g., `BootstrapFewShot`, `SignatureOptimizer`, ...\n",
    "- Maximize `metric` over `trainset` using `optimizer` by adjusting text generation within modules.\n",
    "\n",
    "**DSPy heuristically searches for the most effective strategy to prompt an LLM to achieve the task according to the pipeline.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Pipeline 101\n",
    "\n",
    "(`title & abstract`, `categories`) → pipeline101 → `preds`\n",
    "\n",
    "- Just title & abstract, no text body of the paper.\n",
    "- No custom modules or creative modules usage.\n",
    "- No RAG.\n",
    "\n",
    "(But all the above can be easily added later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Signature\n",
    "\n",
    "**Signatures** are like types in a programming language.\n",
    "\n",
    "- They define the module's input/output.\n",
    "- Their `__doc__` will be included in the LLM prompt, so they can specify the goal of a module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "TWMKdU8eQJu6",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PredictCategories(dspy.Signature):\n",
    "    __doc__ = (\n",
    "        f\"Given the abstract of a scientific paper, \"\n",
    "        f\"identify most relevant categories. \"\n",
    "        f\"Valid categories are {CATEGORIES.keys()}\"\n",
    "    )\n",
    "    title = dspy.InputField()\n",
    "    abstract = dspy.InputField()\n",
    "    categories = dspy.OutputField(\n",
    "        desc=\"list of comma-separated categories\",\n",
    "        format=lambda x: \", \".join(x) if isinstance(x, list) else x,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Pipeline / Module\n",
    "\n",
    "The pipeline is a Module as well.\n",
    "\n",
    "Similar to PyTorch, it makes use of:\n",
    "- `__init__`: Here, the modules are instantiated.\n",
    "- `forward`: Here, it is defined how modules interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "hVrLgbZvbJ97",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Pipeline101(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.predict = dspy.ChainOfThought(PredictCategories)\n",
    "\n",
    "    def forward(self, title, abstract, text=None, labels=None):\n",
    "        categories = self.predict(title=title, abstract=abstract).completions.categories\n",
    "        categories = [cat.strip() for cat in categories[0].split(\",\")]\n",
    "        return dspy.Prediction(categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Language Model\n",
    "\n",
    "The **Language Model (LM)** is at the core of the pipeline.\n",
    "\n",
    "- It is used for processing and generating text in the pipeline.\n",
    "- It is used by the optimizers to improve the pipeline itself.\n",
    "\n",
    "For simple tasks, it can be *fast* and *cheap* (many calls in the optimization).\n",
    "\n",
    "**DSPy caches all the calls to LM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "eBYBiXWZyPjy",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can host local model with ollama.\n",
    "# Just change `model` and `api_base` accordingly.\n",
    "# For example: `model=\"gemma\"` & `api_base=\"http://localhost:11434/v1/\"`\n",
    "lm = dspy.OpenAI(\n",
    "    model=\"gpt3.5-turbo\",\n",
    "    api_base=f\"http://localhost:{PORT_LM}/v1/\",\n",
    "    api_key=\"you-api-key\",\n",
    "    model_type=\"chat\",\n",
    ")\n",
    "\n",
    "# configure dspy to use `lm` as Language Model\n",
    "dspy.settings.configure(lm=lm)\n",
    "\n",
    "# Just testing that LM works\n",
    "lm(\"What's red + yellow?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Optimization\n",
    "\n",
    "As suggest by the [docs](https://dspy-docs.vercel.app/docs/building-blocks/optimizers#which-optimizer-should-i-use), with 50 examples, we choose `BootstrapFewShotWithRandomSearch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is not optimized\n",
    "pipeline101 = Pipeline101()\n",
    "\n",
    "optimizer = BootstrapFewShotWithRandomSearch(\n",
    "    metric=metric_fn,\n",
    "    max_bootstrapped_demos=2,\n",
    "    max_labeled_demos=0,\n",
    "    max_rounds=1,\n",
    "    num_candidate_programs=20,\n",
    "    num_threads=8,\n",
    "    teacher_settings=dict(lm=lm),\n",
    ")\n",
    "\n",
    "pipeline101_optimized = optimizer.compile(\n",
    "    pipeline101,\n",
    "    teacher=pipeline101,\n",
    "    trainset=trainset,\n",
    "    valset=valset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Results\n",
    "\n",
    "We simply compare the `metric_fn` on the `testset`:\n",
    "\n",
    "`pipeline101` *vs.* `pipeline101_optimized`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores_pipeline101 = []\n",
    "for example in testset:\n",
    "    example_x = example.inputs()\n",
    "    example_y = example.labels()\n",
    "    prediction = pipeline101(**example_x)\n",
    "    score = metric_fn(example_y, prediction)\n",
    "    scores_pipeline101.append(score)\n",
    "\n",
    "# Inspcet the last prompt given to LLM\n",
    "lm.inspect_history()\n",
    "print(\"Ground-truth categories:\", example.labels().categories)\n",
    "print(\"Score:\", score)\n",
    "\n",
    "print(\"\\n\" * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores_pipeline101_optimized = []\n",
    "for example in testset:\n",
    "    example_x = example.inputs()\n",
    "    example_y = example.labels()\n",
    "    prediction = pipeline101_optimized(**example_x)\n",
    "    score = metric_fn(example_y, prediction)\n",
    "    scores_pipeline101_optimized.append(score)\n",
    "\n",
    "lm.inspect_history()\n",
    "print(\"Ground-truth categories:\", example.labels().categories)\n",
    "print(\"Score:\", score)\n",
    "print(\"\\n\" * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"pipeline101:\",\n",
    "    sum(scores_pipeline101) / len(scores_pipeline101),\n",
    ")\n",
    "print(\n",
    "    \"pipeline101_optimized:\",\n",
    "    sum(scores_pipeline101_optimized) / len(scores_pipeline101_optimized),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "While developing this notebook, we:\n",
    "- Processed 7,537,982 input tokens (0.0005/1K)\n",
    "- Generated 315,868 output tokens (0.0015/1K)\n",
    "- With an estimated cost of < $5\n",
    "\n",
    "---\n",
    "\n",
    "| Pipeline                   | Avg. metric_fn |\n",
    "|----------------------------|---------------:|\n",
    "| pipeline101                |           56%  |\n",
    "| pipeline101_optimized      |           65%  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Future Work\n",
    "\n",
    "- Add RAG.\n",
    "\n",
    "- Utilize the category descriptions.\n",
    "\n",
    "- Use the full body of the paper.\n",
    "  - Generate summaries.\n",
    "  - Use a sliding window, process chunks, and aggregate.\n",
    "  - Use a more capable language model with greater context length.\n",
    "\n",
    "- Validate data with the `Assert` module.\n",
    "\n",
    "- Use a smarter teacher (e.g., GPT-4).\n",
    "\n",
    "- Experiment with more creative pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Why DSPy?\n",
    "\n",
    "- It has promising core concepts.\n",
    "- It is actively being developed.\n",
    "- It is versatile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Why Not DSPy?\n",
    "\n",
    "- It is not production-ready.\n",
    "- As of 23rd February 2024, it is not well-documented (see [#390](https://github.com/stanfordnlp/dspy/issues/390)).\n",
    "- Other alternatives exist for similar use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Alternatives\n",
    "\n",
    "Many frameworks exist that programmatically generate prompts and parse responses.\n",
    "\n",
    "- [Instructor](https://github.com/jxnl/instructor): Provides structured outputs for Large Language Models (LLMs).\n",
    "- [Guidance](https://github.com/guidance-ai/guidance?tab=readme-ov-file#constrained-generation): A guidance language for controlling large language models.\n",
    "- [LMQL](https://github.com/eth-sri/lmql): A language for constraint-guided and efficient LLM programming.\n",
    "- [Outlines](https://github.com/outlines-dev/outlines): Supports structured text generation.\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Guidance\n",
    "\n",
    "  \"...constrain generation (e.g. with regex and CFGs) as well as to interleave control (conditional, loops) and generation seamlessly.\"\n",
    "\n",
    "  ```python\n",
    "  from guidance import models, select\n",
    "\n",
    "  # load a model\n",
    "  llama2 = models.LlamaCpp(path)\n",
    "\n",
    "  # a simple select between two options\n",
    "  llama2 + f'Do you want a joke or a poem? A ' + select(['joke', 'poem'])\n",
    "  ```\n",
    "\n",
    "  > Do you want a joke or a poem? A **poem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Instructor\n",
    "\n",
    "Validate LLMs outputs to streamline data extraction.\n",
    "\n",
    "```python\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Enables `response_model`\n",
    "client = instructor.patch(OpenAI())\n",
    "\n",
    "\n",
    "class UserDetail(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "user = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=UserDetail,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "assert isinstance(user, UserDetail)\n",
    "assert user.name == \"Jason\"\n",
    "assert user.age == 25\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "/Users/simo/Developer/dspy-arxiv",
   "language": "python",
   "name": "dspy-arxiv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
